# -*- coding: utf-8 -*-
"""LSTM on S and P 500.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Myi4xTmtTYGg93sVoLuFAyMZif8Ru-XN
"""

#pandas is imported to read the csv file and perform preprocessing on the dataset.
import pandas
#matlpotlib is used to visualize the plot
import matplotlib.pyplot as plt
#MinMaxScalar is used to normalize the value before training
from sklearn.preprocessing import MinMaxScaler
#numoy is used to deal with the data after train and split as data will be in form of aray for training and testing.
import numpy as np
#keras has 2 models one is functional and another is sequential
from keras.models import Sequential
#Dense layer is the output layer
from keras.layers import Dense
#LSTM is Long Term Short Term Memory
from keras.layers import LSTM
#library used to calculate the mean square error. For classification accuracy is calculated and for regression mean square error is calculated
import math
from sklearn.metrics import mean_squared_error
#data is read using pandas and output is a dataframe

import pandas_datareader as web
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
plt.style.use("fivethirtyeight")

# get stock data
df = web.DataReader("^GSPC", data_source="yahoo", start="1979-01-01", end="2020-03-19")

#show the data
df

#Get the number of rows and columns in the data set
df.shape

#Visualize the closing price history
plt.figure(figsize=(16,8))
plt.title('S&P 500 Close Price History Jan 1,1979 - March 19,2020')
plt.plot(df['Close'])
plt.xlabel('Date', fontsize=14)
plt.ylabel('Close Price USD ($)', fontsize=14)
plt.show()

#Create a new dataframe with only the 'Close column
data = df.filter(['Close'])

#Convert the dataframe to a numpy array
dataset = data.values

#Get the number of rows to train the model on
training_data_len = math.ceil( len(dataset) * .8 )

training_data_len

#Scale the data: scaling or normalisation 
scaler = MinMaxScaler(feature_range=(0,1))
scaled_data = scaler.fit_transform(dataset)

scaled_data

#Create the training data set
#Create the scaled training data set
train_data = scaled_data[0:training_data_len , :] # since scale_data is an array so with ":" all values are taken

#Split the data into x_train and y_train data sets 
x_train = [] # list data format
y_train = [] # list data format

for i in range(5, len(train_data)):
  x_train.append(train_data[i-5:i, 0])
  y_train.append(train_data[i, 0])
  if i<= 6: #for example visualization
    print(x_train)
    print(y_train)
    print()

#Convert the x_train and y_train to numpy arrays 
x_train, y_train = np.array(x_train), np.array(y_train)
print(x_train)

#Reshape the data: LSTM requires 3D shape of the input dataset
x_train.shape
x_train = np.reshape(x_train, (x_train.shape[0], x_train.shape[1], 1))
x_train.shape

#Build the LSTM model
model = Sequential()
model.add(LSTM(50, return_sequences=True, input_shape= (x_train.shape[1], 1))) # 50 means number of neurons provided 
model.add(LSTM(50, return_sequences= False)) # if more layer make return sequence to true and the last layer is False
model.add(Dense(25))
model.add(Dense(1))

#Compile the model
model.compile(optimizer='adam', loss='mean_squared_error') # loss function check how will a model performed

#Train the model
model.fit(x_train, y_train, batch_size=1, epochs=25) # fit is another name for train, epochs is the number of interations

#Create the testing data set
#Create a new array containing scaled values  
test_data = scaled_data[training_data_len - 5: , :]

#Create the data sets x_test and y_test: y has all the vales we have to predict, x we append last 60 values
x_test = []
y_test = dataset[training_data_len:, :] # store the original values
for i in range(5, len(test_data)):
  x_test.append(test_data[i-5:i, 0])

#Convert the data to a numpy array
x_test = np.array(x_test)

#Reshape the data
x_test = np.reshape(x_test, (x_test.shape[0], x_test.shape[1], 1 ))

#Get the models predicted price values 
predictions = model.predict(x_test)
predictions = scaler.inverse_transform(predictions)

#Get the root mean squared error (RMSE) # evaluate the model using RMSE
rmse = np.sqrt( np.mean( predictions- y_test )**2 )
rmse # sd of redidulas , lower value better model 
print(rmse)

#Plot the data
train = data[:training_data_len]
valid = data[training_data_len:]
valid['Predictions'] = predictions #adding column to valid

#Visualize the data
plt.figure(figsize=(16,8))
plt.title('S&P 500 Jan,1979 - March,2020 - LSTM Model')
plt.xlabel('Date', fontsize=18)
plt.ylabel('S&P 500 Close Price USD ($)', fontsize=18)
plt.plot(train['Close'])
plt.plot(valid[['Close', 'Predictions']])
plt.legend(['Train', 'Actual', 'Predictions'], loc='lower right')
plt.show()

#Show the valid and predicted prices
valid

#Get the quote
apple_quote = web.DataReader('^GSPC', data_source='yahoo', start="1979-01-01", end="2020-03-19")
#Create a new dataframe
new_df = apple_quote.filter(['Close'])

#Get teh last 5 day closing price values and convert the dataframe to an array
last_5_days = new_df[-5:].values

#Scale the data to be values between 0 and 1 using the same variable as before
last_5_days_scaled = scaler.transform(last_5_days)

#Create an empty list
X_test = []
#Append teh past 5 days
X_test.append(last_5_days_scaled)
#Convert the X_test data set to a numpy array
X_test = np.array(X_test)

#Reshape the data
X_test = np.reshape(X_test, (X_test.shape[0], X_test.shape[1], 1))

#Get the predicted scaled price
pred_price = model.predict(X_test)

#undo the scaling 
pred_price = scaler.inverse_transform(pred_price)
print(pred_price)

#Get the quote
apple_quote2 = web.DataReader('^GSPC', data_source='yahoo', start='2020-03-20', end='2020-03-20')
print(apple_quote2['Close'])

valid.to_csv("Predicted and Actual S&P500 values.csv")

